{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This program using to estimate our program\n",
    "#Using trial restaurant.xml only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time is: 0.015631914138793945 seconds\n"
     ]
    }
   ],
   "source": [
    "#Part 1: XML to CSV\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "from time import time\n",
    "import csv\n",
    "                ################################################\n",
    "                ########### 1. Resturant Trial Data ############\n",
    "                ################################################\n",
    "start=time()\n",
    "doc = xml.dom.minidom.parse('/Users/xizhaohan/Desktop/498project2/data/trial/restaurants-trial.xml')\n",
    "docroot = doc.documentElement\n",
    "\n",
    "text=docroot.getElementsByTagName('text')\n",
    "Sentences=[]\n",
    "for sent in text:\n",
    "    Sentences.append(sent.firstChild.data)\n",
    "    \n",
    "    \n",
    "    \n",
    "tree = ET.ElementTree(file='/Users/xizhaohan/Desktop/498project2/data/trial/restaurants-trial.xml')\n",
    "treeroot=tree.getroot()\n",
    "\n",
    "Term_and_Polarity=[]\n",
    "Category_and_Polarity=[]\n",
    "for sentence in treeroot:\n",
    "    tempsent1=''\n",
    "    tempsent2=''\n",
    "    if len(sentence)==3:\n",
    "        for AT in sentence[1]:\n",
    "            Term=AT.attrib\n",
    "            tempsent1+='('+Term['term']+'#'+Term['polarity']+')'\n",
    "        for AC in sentence[2]:\n",
    "            Category=AC.attrib\n",
    "            tempsent2+='('+Category['category']+'#'+Category['polarity']+')'\n",
    "    if len(sentence)==2:\n",
    "        for A in sentence[1]:\n",
    "            Aspect=A.attrib\n",
    "            if 'term' in Aspect:\n",
    "                tempsent1+='('+Aspect['term']+'#'+Aspect['polarity']+')'\n",
    "            else:\n",
    "                tempsent2+='('+Aspect['category']+'#'+Aspect['polarity']+')'\n",
    "\n",
    "    Term_and_Polarity.append(tempsent1)\n",
    "    Category_and_Polarity.append(tempsent2)\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/498estimate/Trial_restaurant_origianlContent.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1,col2,col3 in zip(Term_and_Polarity,Category_and_Polarity,Sentences):\n",
    "        writer.writerow([str(col1),str(col2),str(col3)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('total time is:',stop-start,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 51.47542691230774 seconds\n"
     ]
    }
   ],
   "source": [
    "#Part 2 Get outcomes of subtask2\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################\n",
    "start=time()\n",
    "###########  I-1:read Trial_restaurant_origianlContent.csv ###########\n",
    "#I-1-1: get lists\n",
    "Sent=[]\n",
    "AT=[]\n",
    "AC=[]\n",
    "SentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/498estimate/Trial_restaurant_origianlContent.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "    # row is a list:\n",
    "    #['(AT1#ATP1)(AT2#ATP2)', '(AC1#ACP1)(AC2#ACP2)', 'sent']\n",
    "        Sent.append(row[2])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        ACraw=row[1].strip('()').split(')(')\n",
    "        samesentAT=[tuple(ele1.split('#')) for ele1 in ATraw]\n",
    "        samesentAC=[tuple(ele2.split('#')) for ele2 in ACraw]\n",
    "        AT.append(samesentAT)\n",
    "        AC.append(samesentAC)\n",
    "        SentsNum+=1\n",
    "file.close()\n",
    "\n",
    "#I-1-2: get total dictionary\n",
    "All_dict={}\n",
    "for ID in range(1,SentsNum+1):\n",
    "    #ID in dict form 1 to 100(total number of trial sentences)\n",
    "    All_dict[str(ID)]={'sentence':Sent[ID-1], 'AspectTerm':AT[ID-1], 'AspectCategroy':AC[ID-1]}\n",
    "    #TestAll_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[(AT1,ATP1),(AT2,ATP2)],'AspectCategory':[(AC1,ACP1),(AC2,ACP2)]}, 'ID2':{}, ...}\n",
    "\n",
    "###########  I-2:read 4 CSV files with deleted sentiment adj ###########\n",
    "SentimentDict_Deleted={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/positive_deleted_RST.csv', 'r') as file1:\n",
    "    reader1 = csv.reader(file1)\n",
    "    for row in reader1:\n",
    "        SentimentDict_Deleted['positive']+=row\n",
    "file1.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/negative_deleted_RST.csv', 'r') as file2:\n",
    "    reader2 = csv.reader(file2)\n",
    "    for row in reader2:\n",
    "        SentimentDict_Deleted['negative']+=row\n",
    "file2.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/neutral_deleted_RST.csv', 'r') as file3:\n",
    "    reader3 = csv.reader(file3)\n",
    "    for row in reader3:\n",
    "        SentimentDict_Deleted['neutral']+=row\n",
    "file3.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/conflict_deleted_RST.csv', 'r') as file4:\n",
    "    reader4 = csv.reader(file4)\n",
    "    for row in reader4:\n",
    "        SentimentDict_Deleted['conflict']+=row\n",
    "file4.close()\n",
    "\n",
    "\n",
    "                    ########################################################\n",
    "                    ############  II. Extend 4 sentiment lists  ############\n",
    "                    ########################################################\n",
    "\n",
    "SentimentDict_Extended={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "for adj in SentimentDict_Deleted['positive']:\n",
    "    allsyn=wn.synsets(adj)\n",
    "    for word in allsyn:\n",
    "        SentimentDict_Extended['positive'].append(str(word).strip('Synset()').split('.')[0].strip('\\''))\n",
    "SentimentDict_Extended['positive']=list(set(SentimentDict_Extended['positive']))\n",
    "\n",
    "for adj in SentimentDict_Deleted['negative']:\n",
    "    allsyn=wn.synsets(adj)\n",
    "    for word in allsyn:\n",
    "        SentimentDict_Extended['negative'].append(str(word).strip('Synset()').split('.')[0].strip('\\''))\n",
    "SentimentDict_Extended['negative']=list(set(SentimentDict_Extended['negative']))\n",
    "\n",
    "SentimentDict_Extended['neutral']=SentimentDict_Deleted['neutral']\n",
    "SentimentDict_Extended['conflict']=SentimentDict_Deleted['conflict']\n",
    "\n",
    "                    ###########################################################\n",
    "                    ############  III. Judge polarity of trial AT  ############\n",
    "                    ###########################################################\n",
    "            \n",
    "estimate_subtask2={}\n",
    "for ID in range(1,SentsNum+1):\n",
    "    estimate_subtask2[str(ID)]={'sentence':Sent[ID-1], 'AspectTerm':[], 'AspectCategroy':[]}\n",
    "    for p in AT[ID-1]:\n",
    "        estimate_subtask2[str(ID)]['AspectTerm'].append(p[0])\n",
    "    for p in AC[ID-1]:\n",
    "        estimate_subtask2[str(ID)][ 'AspectCategroy'].append(p[0])\n",
    "    #estimate_subtask2 is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[AT1,AT2],'AspectCategory':[AC1,AC2]}, 'ID2':{}, ...}\n",
    "\n",
    "OutDict_subtask2={}\n",
    "for ID in range(1,SentsNum+1):\n",
    "    #if ID>30:\n",
    "        #break\n",
    "    #III-1: drag element\n",
    "    currentSent=estimate_subtask2[str(ID)]['sentence']\n",
    "    currentAT=estimate_subtask2[str(ID)]['AspectTerm']\n",
    "    currentAC=estimate_subtask2[str(ID)]['AspectCategroy']\n",
    "    OutDict_subtask2[str(ID)]={'sentence':currentSent, 'AspectTerm':[], 'AspectCategroy':currentAC}\n",
    "    if len(currentAT[0])==0:\n",
    "        continue\n",
    "    #III-2 parse each sentence and get all (N,Adj) pairs in list form\n",
    "    Ntag=('NN','NNS','NNP','NNPS')\n",
    "    ADJtag=('JJ','JJR','JJS')\n",
    "    N_Adj_pair=[]\n",
    "    try:\n",
    "        ParseSent=list(dependency_parser.raw_parse(currentSent).__next__().triples())\n",
    "    except BaseException:  continue\n",
    "    #ParseSent looks like:\n",
    "    #[(('horrible', 'JJ'), 'cc', ('But', 'CC')), \n",
    "    #(('horrible', 'JJ'), 'nsubj', ('staff', 'NN')), \n",
    "    #(('staff', 'NN'), 'det', ('the', 'DT')), \n",
    "    #(('to', 'TO'), 'pobj', ('us', 'PRP'))]\n",
    "    for outpair in ParseSent:\n",
    "        if outpair[0][1] in Ntag and outpair[2][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[0][0], outpair[2][0])))\n",
    "        elif outpair[2][1] in Ntag and outpair[0][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[2][0], outpair[0][0])))\n",
    "    #N_Adj_pair looks like:\n",
    "    #[('food', 'fair'), ('factor', 'only'), ('deficiencies', 'other')]\n",
    "    \n",
    "    #III-3: detect whether N is AT\n",
    "    AT_ADJ=[]\n",
    "    for pair in N_Adj_pair:\n",
    "        noun=pair[0]\n",
    "        adj=pair[1]\n",
    "        for at in currentAT:\n",
    "            if noun==at or noun in at:\n",
    "                AT_ADJ.append(tuple((at, adj)))\n",
    "    #AT_ADJ looks like:\n",
    "    #[('sushi', 'best'), ('place', 'clean')]\n",
    "    #its noun is real AT, its adj is candidate\n",
    "    #III-4: detect polarity of AT in AT_ADJ by adj\n",
    "    AspectTerm=[]\n",
    "    for p in AT_ADJ:\n",
    "        if p[1] in SentimentDict_Extended['positive']:\n",
    "            AspectTerm.append(tuple((p[0], 'positive')))\n",
    "        elif p[1] in SentimentDict_Extended['negative']:\n",
    "            AspectTerm.append(tuple((p[0], 'negative')))\n",
    "        elif p[1] in SentimentDict_Extended['conflict']:\n",
    "            AspectTerm.append(tuple((p[0], 'conflict')))\n",
    "        else:\n",
    "            AspectTerm.append(tuple((p[0], 'neutral')))\n",
    "    if len(AT_ADJ)!=0:\n",
    "        OutDict_subtask2[str(ID)]['AspectTerm']=AspectTerm\n",
    "        #OutDict_subtask2 stores the outcome of this program in dictionary form,looks like:\n",
    "        #{'id1':{'sentence':..., 'AspectTerm':[(AT1,ATP1),(AT2,ATP2)], 'AspectCategory':[AC1,AC2]}, 'id2':{}}\n",
    "    else:\n",
    "        blob = TextBlob(currentSent)\n",
    "        tempATP=blob.sentiment.polarity\n",
    "        if tempATP>0:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'positive')))\n",
    "        elif tempATP<0:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'negative')))\n",
    "        else:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'neutral')))\n",
    "        OutDict_subtask2[str(ID)]['AspectTerm']=AspectTerm\n",
    "        \n",
    "    OutDict_subtask2[str(ID)]['AspectTerm']=list(set(OutDict_subtask2[str(ID)]['AspectTerm']))\n",
    "    OutDict_subtask2[str(ID)]['AspectCategroy']=list(set(OutDict_subtask2[str(ID)]['AspectCategroy']))\n",
    "\n",
    "\n",
    "                    ##############################################\n",
    "                    ############  IV. Write into CSV  ############\n",
    "                    ##############################################               \n",
    "AT_Write=[]\n",
    "AC_Write=[]\n",
    "Sent_Write=[]\n",
    "for ID in range(1,SentsNum+1):\n",
    "    #if ID>30:\n",
    "        #break\n",
    "    Sent_Write.append(OutDict_subtask2[str(ID)]['sentence'])\n",
    "    sameSentAT=''\n",
    "    sameSentAC=''\n",
    "    for ele1 in OutDict_subtask2[str(ID)]['AspectTerm']:\n",
    "        sameSentAT+='('+ele1[0]+'#'+ele1[1]+')'\n",
    "    for ele2 in OutDict_subtask2[str(ID)]['AspectCategroy']:\n",
    "        sameSentAC+='('+ele2+')'\n",
    "    AT_Write.append(sameSentAT)\n",
    "    AC_Write.append(sameSentAC) \n",
    "    \n",
    "with open( '/Users/xizhaohan/Desktop/498estimate/Subtask2estimate_restaurant.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1,col2,col3 in zip(AT_Write,AC_Write,Sent_Write):\n",
    "        writer.writerow([str(col1),str(col2),str(col3)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:', stop-start,'seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 0.01382303237915039 seconds\n"
     ]
    }
   ],
   "source": [
    "#Part 3 Get outcomes of subtask4\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "start=time()\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################   \n",
    "\n",
    "#I-1: read Subtask2estimate_restaurant.csv\n",
    "#I-1-1: get lists\n",
    "task2Sent=[]\n",
    "task2AT=[]\n",
    "task2AC=[]\n",
    "task2SentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/498estimate/Subtask2estimate_restaurant.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "    # row is a list:\n",
    "    #['(AT1#ATP1)(AT2#ATP2)', '(AC1)(AC2)', 'sent']\n",
    "        task2Sent.append(row[2])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        ACraw=row[1].strip('()').split(')(')\n",
    "        samesentAT=[tuple(ele1.split('#')) for ele1 in ATraw]\n",
    "        samesentAC=[ele2 for ele2 in ACraw]\n",
    "        task2AT.append(samesentAT)\n",
    "        task2AC.append(samesentAC)\n",
    "        task2SentsNum+=1\n",
    "file.close()\n",
    "\n",
    "#I-1-2: get total dictionary\n",
    "Task2All_dict={}\n",
    "for ID in range(1,task2SentsNum+1):\n",
    "    #ID in dict form 1 to 100(total number of trial sentences)\n",
    "    Task2All_dict[str(ID)]={'sentence':task2Sent[ID-1], 'AspectTerm':task2AT[ID-1], 'AspectCategroy':task2AC[ID-1]}\n",
    "    #Task2All_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[(AT1,ATP1),(AT2,ATP2)],'AspectCategory':[AC1,AC2]}, 'ID2':{}, ...}\n",
    "\n",
    "#I-2: read AT-AC.csv\n",
    "#I-2-1: get lists\n",
    "AT2AC_AT=[]\n",
    "AT2AC_AC=[]\n",
    "AT2AC_Num=0\n",
    "Label_AC={'1':'food', '2':'service', '3':'price', '4':'ambience', '5':'anecdotes/miscellaneous', '':'anecdotes/miscellaneous'}\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/AT_AC.csv', 'r') as file_AT2AC:\n",
    "    reader_AT2AC = csv.reader(file_AT2AC)\n",
    "    for row in reader_AT2AC:\n",
    "    # row is a list:\n",
    "    #['(AT1)(AT2)', 'AClabel', 'sent'] AClabel is 1,2,3,4,5\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        samesentAT=[ele for ele in ATraw]\n",
    "        AT2AC_AT.append(samesentAT)\n",
    "        AT2AC_AC.append(Label_AC[row[1]])\n",
    "        AT2AC_Num+=1\n",
    "file_AT2AC.close()\n",
    "\n",
    "#I-1-2: get dictionary \n",
    "AT2AC_Dict={'food':[], 'service':[], 'price':[], 'ambience':[],'anecdotes/miscellaneous':[] }\n",
    "for i in range(AT2AC_Num):\n",
    "    AT2AC_Dict[AT2AC_AC[i]]+=AT2AC_AT[i]\n",
    "    #AT2AC_Dict is a dictionray with 5 AC lists, in which contain the corresponding AT.\n",
    "\n",
    "                    ########################################################\n",
    "                    ############  II. Get outcomes of subtask4  ############\n",
    "                    ########################################################\n",
    "\n",
    "FinalDict_RST={}\n",
    "cnt=0\n",
    "for ID in range(1, task2SentsNum+1):\n",
    "    FinalDict_RST[str(ID)]={'sentence':Task2All_dict[str(ID)]['sentence'], 'AspectTerm':[], 'AspectCategroy':[]}\n",
    "    if len(Task2All_dict[str(ID)]['AspectTerm'][0][0])!=0:\n",
    "        FinalDict_RST[str(ID)]['AspectTerm']=Task2All_dict[str(ID)]['AspectTerm']\n",
    "        ATP_SameSent=[pair[1] for pair in Task2All_dict[str(ID)]['AspectTerm']]\n",
    "        if len(list(set(ATP_SameSent)))==1:\n",
    "            ACP=ATP_SameSent[0]\n",
    "            AC_P_SameSent=[tuple((ac, ACP)) for ac in Task2All_dict[str(ID)]['AspectCategroy']]\n",
    "            FinalDict_RST[str(ID)]['AspectCategroy']=AC_P_SameSent\n",
    "        elif len(list(set(ATP_SameSent)))>1:\n",
    "            AC_P_SameSent=[]\n",
    "            for at in Task2All_dict[str(ID)]['AspectTerm']:\n",
    "                currentAC=''\n",
    "                if at[0] in AT2AC_Dict['food']:\n",
    "                    currentAC='food'\n",
    "                elif at[0] in AT2AC_Dict['service']:\n",
    "                    currentAC='service'\n",
    "                elif at[0] in AT2AC_Dict['price']:\n",
    "                    currentAC='price'\n",
    "                elif at[0] in AT2AC_Dict['ambience']:\n",
    "                    currentAC='ambience'\n",
    "                else :\n",
    "                    currentAC='anecdotes/miscellaneous'\n",
    "                currentACP=at[1]\n",
    "                AC_P_SameSent.append(tuple((currentAC, currentACP)))\n",
    "            AC_P_SameSent=list(set(AC_P_SameSent))\n",
    "            FinalDict_RST[str(ID)]['AspectCategroy']=AC_P_SameSent\n",
    "    else:\n",
    "        blob = TextBlob(Task2All_dict[str(ID)]['sentence'])\n",
    "        tempACP=blob.sentiment.polarity\n",
    "        AC_P_SameSent=[]\n",
    "        if tempACP>0:\n",
    "            for ac in Task2All_dict[str(ID)]['AspectCategroy']:\n",
    "                AC_P_SameSent.append(tuple((ac, 'positive')))\n",
    "        elif tempACP<0:\n",
    "            for ac in Task2All_dict[str(ID)]['AspectCategroy']:\n",
    "                AC_P_SameSent.append(tuple((ac, 'negatigve')))\n",
    "        else:\n",
    "            for ac in Task2All_dict[str(ID)]['AspectCategroy']:\n",
    "                AC_P_SameSent.append(tuple((ac, 'neutral')))\n",
    "        FinalDict_RST[str(ID)]['AspectCategroy']=AC_P_SameSent\n",
    "    FinalDict_RST[str(ID)]['AspectTerm']=list(set(FinalDict_RST[str(ID)]['AspectTerm']))\n",
    "    FinalDict_RST[str(ID)]['AspectCategroy']=list(set(FinalDict_RST[str(ID)]['AspectCategroy']))\n",
    "                    \n",
    "                    ###############################################\n",
    "                    ############  III. Write into CSV  ############\n",
    "                    ###############################################              \n",
    "AT_Write=[]\n",
    "AC_Write=[]\n",
    "Sent_Write=[]\n",
    "for ID in range(1,task2SentsNum+1):\n",
    "    Sent_Write.append(FinalDict_RST[str(ID)]['sentence'])\n",
    "    sameSentAT=''\n",
    "    sameSentAC=''\n",
    "    for ele1 in FinalDict_RST[str(ID)]['AspectTerm']:\n",
    "        sameSentAT+='('+ele1[0]+'#'+ele1[1]+')'\n",
    "    for ele2 in FinalDict_RST[str(ID)]['AspectCategroy']:\n",
    "        sameSentAC+='('+ele2[0]+'#'+ele2[1]+')'\n",
    "    AT_Write.append(sameSentAT)\n",
    "    AC_Write.append(sameSentAC)\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/498estimate/Subtask4_FinalOutcome_trial_restaurant.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1,col2,col3 in zip(AT_Write,AC_Write,Sent_Write):\n",
    "        writer.writerow([str(col1),str(col2),str(col3)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:', stop-start,'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we can count the correct estimate by eyes \n",
    "#between Trial_restaurant_origianlContent.csv and Subtask4_FinalOutcome_trial_restaurant.csv\n",
    "#after all, there are only 100 sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
