{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/nltk/parse/dependencygraph.py:378: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 2963.4731209278107 seconds\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#Part I of this big program\n",
    "#This program for subtask 2 on laptop only\n",
    "#We only read Train_laptop.csv \n",
    "#we will get 4 csv files containing sentiment adj \n",
    "#then delete some error adj by hand and carry on next program\n",
    "##############################################################\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################\n",
    "start=time()\n",
    "###########  read Train_laptop.csv ###########\n",
    "#I-1: get lists\n",
    "trainSent=[]\n",
    "trainAT=[]\n",
    "trainSentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/XML_to_CSV/Train_laptop.csv', 'r') as file_trainLPT:\n",
    "    reader_trainLPT = csv.reader(file_trainLPT)\n",
    "    for row in reader_trainLPT:\n",
    "    # row is a list:\n",
    "    #['(AT1#ATP1)(AT2#ATP2)', 'sent']\n",
    "        trainSent.append(row[1])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        samesentAT=[tuple(ele1.split('#')) for ele1 in ATraw]\n",
    "        trainAT.append(samesentAT)\n",
    "        #trainAT are 3-D structure:\n",
    "        #most inside-D is ('AT','ATP') pair\n",
    "        #middle D is pair from same sentence: [(pair),(pair)]\n",
    "        #outside looks like:[[same sent],[same sent]]\n",
    "        trainSentsNum+=1\n",
    "file_trainLPT.close()\n",
    "\n",
    "#I-2: get total dictionary\n",
    "TrainAll_dict={}\n",
    "for ID in range(1,trainSentsNum+1):\n",
    "    #ID in dict form 1 to 3041(total number of training sentences)\n",
    "    TrainAll_dict[str(ID)]={'sentence':trainSent[ID-1], 'AspectTerm':trainAT[ID-1]}\n",
    "    #TrainAll_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[(AT1,ATP1),(AT2,ATP2)]}, 'ID2':{}, ...}\n",
    "                  \n",
    "                #######################################################\n",
    "                ############  II. Get 4 sentiment classes  ############\n",
    "                #######################################################\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "SentimentDict={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "\n",
    "for ID in range(1,trainSentsNum+1):\n",
    "    #if ID>5:\n",
    "        #break\n",
    "    #II-1: drag element from trainAll_dict\n",
    "    currentSent=TrainAll_dict[str(ID)]['sentence']\n",
    "    currentAT_and_P=TrainAll_dict[str(ID)]['AspectTerm']\n",
    "    if len(currentAT_and_P[0])>1:\n",
    "        currentAT=[p[0] for p in currentAT_and_P] #store all AT in current sentence, 1-D list\n",
    "        currentATP=[p[1] for p in currentAT_and_P] #store all ATP in current sentence, 1-D list\n",
    "        currentAT_P_dict={} #store all AT and ATP in {'AT': ATP} form, 1-D dictionary\n",
    "        for at_p_pair in currentAT_and_P:\n",
    "            currentAT_P_dict[at_p_pair[0]]=at_p_pair[1]\n",
    "    else:\n",
    "        currentAT=[]\n",
    "        currentATP=[]\n",
    "        currentAT_P_dict={}\n",
    "        \n",
    "    #II-2 parse each sentence and get all (N,Adj) pairs in list form\n",
    "    Ntag=('NN','NNS','NNP','NNPS')\n",
    "    ADJtag=('JJ','JJR','JJS')\n",
    "    N_Adj_pair=[]\n",
    "    try:\n",
    "        ParseSent=list(dependency_parser.raw_parse(currentSent).__next__().triples())\n",
    "    except BaseException:  continue\n",
    "    #ParseSent looks like:\n",
    "    #[(('horrible', 'JJ'), 'cc', ('But', 'CC')), \n",
    "    #(('horrible', 'JJ'), 'nsubj', ('staff', 'NN')), \n",
    "    #(('staff', 'NN'), 'det', ('the', 'DT')), \n",
    "    #(('to', 'TO'), 'pobj', ('us', 'PRP'))]\n",
    "    for outpair in ParseSent:\n",
    "        if outpair[0][1] in Ntag and outpair[2][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[0][0], outpair[2][0])))\n",
    "        elif outpair[2][1] in Ntag and outpair[0][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[2][0], outpair[0][0])))\n",
    "    #N_Adj_pair looks like:\n",
    "    #[('food', 'fair'), ('factor', 'only'), ('deficiencies', 'other')]\n",
    "    \n",
    "    #II-3: detect adj\n",
    "    if len(set(currentATP))==1: #len of set(currentATP) is kinds of polarity of AT, is 0,1 or 2\n",
    "        SentimentDict[currentATP[0]]+=[ele[1] for ele in N_Adj_pair]\n",
    "    elif len(set(currentATP))>1:\n",
    "        for pair in N_Adj_pair:\n",
    "            noun=pair[0]\n",
    "            adj=pair[1]\n",
    "            for at in currentAT:\n",
    "                if noun==at or noun in at:\n",
    "                    SentimentDict[currentAT_P_dict[at]].append(adj)\n",
    "for key in SentimentDict:\n",
    "    SentimentDict[key]=list(set(SentimentDict[key]))\n",
    "\n",
    "#II-4: write into csv file and delete by hand\n",
    "#only delete some adj obviously not in this sentiment\n",
    "#then use for enxtend\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/positive_not_enxtend_from_trainLPT.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['positive']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/negative_not_enxtend_from_trainLPT.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['negative']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/neutral_not_enxtend_from_trainLPT.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['neutral']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/conflict_not_enxtend_from_trainLPT.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['conflict']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:',stop-start,'seconds')\n",
    "#this part of program end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 404.71827507019043 seconds\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#Part II of this big program\n",
    "#This program for subtask 2 on laptop only\n",
    "#We read Test_laptop_PhaseB.csv and 4 deleted CSV files\n",
    "##############################################################\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################\n",
    "start=time()\n",
    "###########  I-1:read Test_laptop.csv ###########\n",
    "#I-1-1: get lists\n",
    "testSent=[]\n",
    "testAT=[]\n",
    "testSentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/XML_to_CSV/Test_laptop_PhaseB.csv', 'r') as file_testLPT:\n",
    "    reader_testLPT = csv.reader(file_testLPT)\n",
    "    for row in reader_testLPT:\n",
    "    # row is a list:\n",
    "    #['(AT1)(AT2)', 'sent']\n",
    "        testSent.append(row[1])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        samesentAT=[ele1 for ele1 in ATraw]\n",
    "        testAT.append(samesentAT)\n",
    "        #testAT are 2-D structure:\n",
    "        #inside D is AT from same sentence: [AT1,AT2,...]\n",
    "        #outside looks like:[[same sent],[same sent]]\n",
    "        testSentsNum+=1\n",
    "file_testLPT.close()\n",
    "\n",
    "#I-1-2: get total dictionary\n",
    "TestAll_dict={}\n",
    "for ID in range(1,testSentsNum+1):\n",
    "    #ID in dict form 1 to 800(total number of testing sentences)\n",
    "    TestAll_dict[str(ID)]={'sentence':testSent[ID-1], 'AspectTerm':testAT[ID-1]}\n",
    "    #TestAll_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[AT1,AT2]}, 'ID2':{}, ...}\n",
    "\n",
    "###########  I-2:read 4 CSV files with deleted sentiment adj ###########\n",
    "SentimentDict_Deleted={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/positive_deleted_LPT.csv', 'r') as file1:\n",
    "    reader1 = csv.reader(file1)\n",
    "    for row in reader1:\n",
    "        SentimentDict_Deleted['positive']+=row\n",
    "file1.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/negative_deleted_LPT.csv', 'r') as file2:\n",
    "    reader2 = csv.reader(file2)\n",
    "    for row in reader2:\n",
    "        SentimentDict_Deleted['negative']+=row\n",
    "file2.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/neutral_deleted_LPT.csv', 'r') as file3:\n",
    "    reader3 = csv.reader(file3)\n",
    "    for row in reader3:\n",
    "        SentimentDict_Deleted['neutral']+=row\n",
    "file3.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/conflict_deleted_LPT.csv', 'r') as file4:\n",
    "    reader4 = csv.reader(file4)\n",
    "    for row in reader4:\n",
    "        SentimentDict_Deleted['conflict']+=row\n",
    "file4.close()\n",
    "\n",
    "\n",
    "                    ########################################################\n",
    "                    ############  II. Extend 4 sentiment lists  ############\n",
    "                    ########################################################\n",
    "\n",
    "SentimentDict_Extended={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "for adj in SentimentDict_Deleted['positive']:\n",
    "    allsyn=wn.synsets(adj)\n",
    "    for word in allsyn:\n",
    "        SentimentDict_Extended['positive'].append(str(word).strip('Synset()').split('.')[0].strip('\\''))\n",
    "SentimentDict_Extended['positive']=list(set(SentimentDict_Extended['positive']))\n",
    "\n",
    "for adj in SentimentDict_Deleted['negative']:\n",
    "    allsyn=wn.synsets(adj)\n",
    "    for word in allsyn:\n",
    "        SentimentDict_Extended['negative'].append(str(word).strip('Synset()').split('.')[0].strip('\\''))\n",
    "SentimentDict_Extended['negative']=list(set(SentimentDict_Extended['negative']))\n",
    "\n",
    "SentimentDict_Extended['neutral']=SentimentDict_Deleted['neutral']\n",
    "SentimentDict_Extended['conflict']=SentimentDict_Deleted['conflict']\n",
    "\n",
    "                    ##########################################################\n",
    "                    ############  III. Judge polarity of test AT  ############\n",
    "                    ##########################################################\n",
    "\n",
    "OutTestLPTDict={}\n",
    "for ID in range(1,testSentsNum+1):\n",
    "    #if ID>30:\n",
    "        #break\n",
    "    #III-1: drag element from testAll_dict\n",
    "    currentSent=TestAll_dict[str(ID)]['sentence']\n",
    "    currentAT=TestAll_dict[str(ID)]['AspectTerm']\n",
    "    OutTestLPTDict[str(ID)]={'sentence':currentSent, 'AspectTerm':[]}\n",
    "    if len(currentAT[0])==0:\n",
    "        continue\n",
    "    #III-2 parse each sentence and get all (N,Adj) pairs in list form\n",
    "    Ntag=('NN','NNS','NNP','NNPS')\n",
    "    ADJtag=('JJ','JJR','JJS')\n",
    "    N_Adj_pair=[]\n",
    "    try:\n",
    "        ParseSent=list(dependency_parser.raw_parse(currentSent).__next__().triples())\n",
    "    except BaseException:  continue\n",
    "    #ParseSent looks like:\n",
    "    #[(('horrible', 'JJ'), 'cc', ('But', 'CC')), \n",
    "    #(('horrible', 'JJ'), 'nsubj', ('staff', 'NN')), \n",
    "    #(('staff', 'NN'), 'det', ('the', 'DT')), \n",
    "    #(('to', 'TO'), 'pobj', ('us', 'PRP'))]\n",
    "    for outpair in ParseSent:\n",
    "        if outpair[0][1] in Ntag and outpair[2][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[0][0], outpair[2][0])))\n",
    "        elif outpair[2][1] in Ntag and outpair[0][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[2][0], outpair[0][0])))\n",
    "    #N_Adj_pair looks like:\n",
    "    #[('food', 'fair'), ('factor', 'only'), ('deficiencies', 'other')]\n",
    "    \n",
    "    #III-3: detect whether N is AT\n",
    "    AT_ADJ=[]\n",
    "    for pair in N_Adj_pair:\n",
    "        noun=pair[0]\n",
    "        adj=pair[1]\n",
    "        for at in currentAT:\n",
    "            if noun==at or noun in at:\n",
    "                AT_ADJ.append(tuple((at, adj)))\n",
    "    #AT_ADJ looks like:\n",
    "    #[('sushi', 'best'), ('place', 'clean')]\n",
    "    #its noun is real AT, its adj is candidate\n",
    "    #III-4: detect polarity of AT in AT_ADJ by adj\n",
    "    AspectTerm=[]\n",
    "    for p in AT_ADJ:\n",
    "        if p[1] in SentimentDict_Extended['positive']:\n",
    "            AspectTerm.append(tuple((p[0], 'positive')))\n",
    "        elif p[1] in SentimentDict_Extended['negative']:\n",
    "            AspectTerm.append(tuple((p[0], 'negative')))\n",
    "        elif p[1] in SentimentDict_Extended['conflict']:\n",
    "            AspectTerm.append(tuple((p[0], 'conflict')))\n",
    "        else:\n",
    "            AspectTerm.append(tuple((p[0], 'neutral')))\n",
    "    if len(AT_ADJ)!=0:\n",
    "        OutTestLPTDict[str(ID)]['AspectTerm']=AspectTerm\n",
    "        #OutTestRSTDict store the outcome of this program in dictionary form,looks like:\n",
    "        #{'id1':{'sentence':..., 'AspectTerm':[(AT1,ATP1),(AT2,ATP2)], 'AspectCategory':[AC1,AC2]}, 'id2':{}}\n",
    "    else:\n",
    "        blob = TextBlob(currentSent)\n",
    "        tempATP=blob.sentiment.polarity\n",
    "        if tempATP>0:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'positive')))\n",
    "        elif tempATP<0:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'negative')))\n",
    "        else:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'neutral')))\n",
    "        OutTestLPTDict[str(ID)]['AspectTerm']=AspectTerm   \n",
    "    OutTestLPTDict[str(ID)]['AspectTerm']=list(set(OutTestLPTDict[str(ID)]['AspectTerm']))\n",
    "    \n",
    "                    ##############################################\n",
    "                    ############  IV. Write into CSV  ############\n",
    "                    ##############################################               \n",
    "AT_Write=[]\n",
    "Sent_Write=[]\n",
    "for ID in range(1,testSentsNum+1):\n",
    "    #if ID>30:\n",
    "        #break\n",
    "    Sent_Write.append(OutTestLPTDict[str(ID)]['sentence'])\n",
    "    sameSentAT=''\n",
    "    for ele1 in OutTestLPTDict[str(ID)]['AspectTerm']:\n",
    "        sameSentAT+='('+ele1[0]+'#'+ele1[1]+')'\n",
    "    AT_Write.append(sameSentAT)\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/Subtask2_FinalOutcome_laptop.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1,col2 in zip(AT_Write,Sent_Write):\n",
    "        writer.writerow([str(col1),str(col2)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:', stop-start,'seconds')\n",
    "\n",
    "#Subtask2 for laptop test PhaseB is finished\n",
    "#we got an CSV file with polarities of AT\n",
    "#this is the final outcome for laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
