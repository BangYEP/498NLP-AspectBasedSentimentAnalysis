{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/nltk/parse/dependencygraph.py:378: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n",
      "//anaconda/lib/python3.5/site-packages/nltk/parse/dependencygraph.py:378: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n",
      "//anaconda/lib/python3.5/site-packages/nltk/parse/dependencygraph.py:378: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 2847.0718660354614 seconds\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#Part I of this big program\n",
    "#This program for subtask 2 on restaurant only\n",
    "#We only read Train_restaurant.csv \n",
    "#we will get 4 csv files containing sentiment adj \n",
    "#then delete some error adj by hand and carry on next program\n",
    "##############################################################\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################\n",
    "start=time()\n",
    "###########  read Train_restaurant.csv ###########\n",
    "#I-1: get lists\n",
    "trainSent=[]\n",
    "trainAT=[]\n",
    "trainAC=[]\n",
    "trainSentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/XML_to_CSV/Train_restaurant.csv', 'r') as file_trainRST:\n",
    "    reader_trainRST = csv.reader(file_trainRST)\n",
    "    for row in reader_trainRST:\n",
    "    # row is a list:\n",
    "    #['(AT1#ATP1)(AT2#ATP2)', '(AC1#ACP1)(AC2#ACP2)', 'sent']\n",
    "        trainSent.append(row[2])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        ACraw=row[1].strip('()').split(')(')\n",
    "        samesentAT=[tuple(ele1.split('#')) for ele1 in ATraw]\n",
    "        samesentAC=[tuple(ele2.split('#')) for ele2 in ACraw]\n",
    "        trainAT.append(samesentAT)\n",
    "        trainAC.append(samesentAC)\n",
    "        #trainAC&trainAT are 3-D structure:\n",
    "        #most inside-D is ('AT/AC','ATP/ACP') pair\n",
    "        #middle D is pair from same sentence: [(pair),(pair)]\n",
    "        #outside looks like:[[same sent],[same sent]]\n",
    "        trainSentsNum+=1\n",
    "file_trainRST.close()\n",
    "\n",
    "#I-2: get total dictionary\n",
    "TrainAll_dict={}\n",
    "for ID in range(1,trainSentsNum+1):\n",
    "    #ID in dict form 1 to 3041(total number of training sentences)\n",
    "    TrainAll_dict[str(ID)]={'sentence':trainSent[ID-1], 'AspectTerm':trainAT[ID-1], 'AspectCategroy':trainAC[ID-1]}\n",
    "    #TrainAll_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[(AT1,ATP1),(AT2,ATP2)],'AspectCategory':[(AC1,ACP1),(AC2,ACP2)]}, 'ID2':{}, ...}\n",
    "\n",
    "                    \n",
    "                #######################################################\n",
    "                ############  II. Get 4 sentiment classes  ############\n",
    "                #######################################################\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "SentimentDict={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "\n",
    "for ID in range(1,trainSentsNum+1):\n",
    "    #if ID>5:\n",
    "        #break\n",
    "    #II-1: drag element from trainAll_dict\n",
    "    currentSent=TrainAll_dict[str(ID)]['sentence']\n",
    "    currentAT_and_P=TrainAll_dict[str(ID)]['AspectTerm']\n",
    "    if len(currentAT_and_P[0])>1:\n",
    "        currentAT=[p[0] for p in currentAT_and_P] #store all AT in current sentence, 1-D list\n",
    "        currentATP=[p[1] for p in currentAT_and_P] #store all ATP in current sentence, 1-D list\n",
    "        currentAT_P_dict={} #store all AT and ATP in {'AT': ATP} form, 1-D dictionary\n",
    "        for at_p_pair in currentAT_and_P:\n",
    "            currentAT_P_dict[at_p_pair[0]]=at_p_pair[1]\n",
    "    else:\n",
    "        currentAT=[]\n",
    "        currentATP=[]\n",
    "        currentAT_P_dict={}\n",
    "        \n",
    "    #II-2 parse each sentence and get all (N,Adj) pairs in list form\n",
    "    Ntag=('NN','NNS','NNP','NNPS')\n",
    "    ADJtag=('JJ','JJR','JJS')\n",
    "    N_Adj_pair=[]\n",
    "    try:\n",
    "        ParseSent=list(dependency_parser.raw_parse(currentSent).__next__().triples())\n",
    "    except BaseException:  continue\n",
    "    #ParseSent looks like:\n",
    "    #[(('horrible', 'JJ'), 'cc', ('But', 'CC')), \n",
    "    #(('horrible', 'JJ'), 'nsubj', ('staff', 'NN')), \n",
    "    #(('staff', 'NN'), 'det', ('the', 'DT')), \n",
    "    #(('to', 'TO'), 'pobj', ('us', 'PRP'))]\n",
    "    for outpair in ParseSent:\n",
    "        if outpair[0][1] in Ntag and outpair[2][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[0][0], outpair[2][0])))\n",
    "        elif outpair[2][1] in Ntag and outpair[0][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[2][0], outpair[0][0])))\n",
    "    #N_Adj_pair looks like:\n",
    "    #[('food', 'fair'), ('factor', 'only'), ('deficiencies', 'other')]\n",
    "    \n",
    "    #II-3: detect adj\n",
    "    if len(set(currentATP))==1: #len of set(currentATP) is kinds of polarity of AT, is 0,1 or 2\n",
    "        SentimentDict[currentATP[0]]+=[ele[1] for ele in N_Adj_pair]\n",
    "    elif len(set(currentATP))>1:\n",
    "        for pair in N_Adj_pair:\n",
    "            noun=pair[0]\n",
    "            adj=pair[1]\n",
    "            for at in currentAT:\n",
    "                if noun==at or noun in at:\n",
    "                    SentimentDict[currentAT_P_dict[at]].append(adj)\n",
    "for key in SentimentDict:\n",
    "    SentimentDict[key]=list(set(SentimentDict[key]))\n",
    "\n",
    "#II-4: write into csv file and delete by hand\n",
    "#only delete some adj obviously not in this sentiment\n",
    "#then use for enxtend\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/positive_not_enxtend_from_trainRST.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['positive']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/negative_not_enxtend_from_trainRST.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['negative']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/neutral_not_enxtend_from_trainRST.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['neutral']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/conflict_not_enxtend_from_trainRST.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1 in SentimentDict['conflict']:\n",
    "        writer.writerow([str(col1)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:',stop-start,'seconds')\n",
    "#this part of program end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 590.6852700710297 seconds\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#Part II of this big program\n",
    "#This program for subtask 2 on restaurant only\n",
    "#We read Test_resturant_PhaseB.csv and 4 deleted CSV files\n",
    "##############################################################\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################\n",
    "start=time()\n",
    "###########  I-1:read Test_restaurant.csv ###########\n",
    "#I-1-1: get lists\n",
    "testSent=[]\n",
    "testAT=[]\n",
    "testAC=[]\n",
    "testSentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/XML_to_CSV/Test_restaurant_PhaseB.csv', 'r') as file_testRST:\n",
    "    reader_testRST = csv.reader(file_testRST)\n",
    "    for row in reader_testRST:\n",
    "    # row is a list:\n",
    "    #['(AT1)(AT2)', '(AC1)(AC2)', 'sent']\n",
    "        testSent.append(row[2])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        ACraw=row[1].strip('()').split(')(')\n",
    "        samesentAT=[ele1 for ele1 in ATraw]\n",
    "        samesentAC=[ele2 for ele2 in ACraw]\n",
    "        testAT.append(samesentAT)\n",
    "        testAC.append(samesentAC)\n",
    "        #testAC&testAT are 2-D structure:\n",
    "        #inside D is AT/AC from same sentence: [AT1,AT2,...]\n",
    "        #outside looks like:[[same sent],[same sent]]\n",
    "        testSentsNum+=1\n",
    "file_testRST.close()\n",
    "\n",
    "#I-1-2: get total dictionary\n",
    "TestAll_dict={}\n",
    "for ID in range(1,testSentsNum+1):\n",
    "    #ID in dict form 1 to 800(total number of testing sentences)\n",
    "    TestAll_dict[str(ID)]={'sentence':testSent[ID-1], 'AspectTerm':testAT[ID-1], 'AspectCategroy':testAC[ID-1]}\n",
    "    #TestAll_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[AT1,AT2],'AspectCategory':[AC1,AC2]}, 'ID2':{}, ...}\n",
    "\n",
    "###########  I-2:read 4 CSV files with deleted sentiment adj ###########\n",
    "SentimentDict_Deleted={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/positive_deleted_RST.csv', 'r') as file1:\n",
    "    reader1 = csv.reader(file1)\n",
    "    for row in reader1:\n",
    "        SentimentDict_Deleted['positive']+=row\n",
    "file1.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/negative_deleted_RST.csv', 'r') as file2:\n",
    "    reader2 = csv.reader(file2)\n",
    "    for row in reader2:\n",
    "        SentimentDict_Deleted['negative']+=row\n",
    "file2.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/neutral_deleted_RST.csv', 'r') as file3:\n",
    "    reader3 = csv.reader(file3)\n",
    "    for row in reader3:\n",
    "        SentimentDict_Deleted['neutral']+=row\n",
    "file3.close()\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/conflict_deleted_RST.csv', 'r') as file4:\n",
    "    reader4 = csv.reader(file4)\n",
    "    for row in reader4:\n",
    "        SentimentDict_Deleted['conflict']+=row\n",
    "file4.close()\n",
    "\n",
    "\n",
    "                    ########################################################\n",
    "                    ############  II. Extend 4 sentiment lists  ############\n",
    "                    ########################################################\n",
    "\n",
    "SentimentDict_Extended={'positive':[],'negative':[], 'neutral':[] , 'conflict':[]}\n",
    "for adj in SentimentDict_Deleted['positive']:\n",
    "    allsyn=wn.synsets(adj)\n",
    "    for word in allsyn:\n",
    "        SentimentDict_Extended['positive'].append(str(word).strip('Synset()').split('.')[0].strip('\\''))\n",
    "SentimentDict_Extended['positive']=list(set(SentimentDict_Extended['positive']))\n",
    "\n",
    "for adj in SentimentDict_Deleted['negative']:\n",
    "    allsyn=wn.synsets(adj)\n",
    "    for word in allsyn:\n",
    "        SentimentDict_Extended['negative'].append(str(word).strip('Synset()').split('.')[0].strip('\\''))\n",
    "SentimentDict_Extended['negative']=list(set(SentimentDict_Extended['negative']))\n",
    "\n",
    "SentimentDict_Extended['neutral']=SentimentDict_Deleted['neutral']\n",
    "SentimentDict_Extended['conflict']=SentimentDict_Deleted['conflict']\n",
    "\n",
    "                    ##########################################################\n",
    "                    ############  III. Judge polarity of test AT  ############\n",
    "                    ##########################################################\n",
    "\n",
    "OutTestRSTDict={}\n",
    "for ID in range(1,testSentsNum+1):\n",
    "    #if ID>30:\n",
    "        #break\n",
    "    #III-1: drag element from testAll_dict\n",
    "    currentSent=TestAll_dict[str(ID)]['sentence']\n",
    "    currentAT=TestAll_dict[str(ID)]['AspectTerm']\n",
    "    currentAC=TestAll_dict[str(ID)]['AspectCategroy']\n",
    "    OutTestRSTDict[str(ID)]={'sentence':currentSent, 'AspectTerm':[], 'AspectCategroy':currentAC}\n",
    "    if len(currentAT[0])==0:\n",
    "        continue\n",
    "    #III-2 parse each sentence and get all (N,Adj) pairs in list form\n",
    "    Ntag=('NN','NNS','NNP','NNPS')\n",
    "    ADJtag=('JJ','JJR','JJS')\n",
    "    N_Adj_pair=[]\n",
    "    try:\n",
    "        ParseSent=list(dependency_parser.raw_parse(currentSent).__next__().triples())\n",
    "    except BaseException:  continue\n",
    "    #ParseSent looks like:\n",
    "    #[(('horrible', 'JJ'), 'cc', ('But', 'CC')), \n",
    "    #(('horrible', 'JJ'), 'nsubj', ('staff', 'NN')), \n",
    "    #(('staff', 'NN'), 'det', ('the', 'DT')), \n",
    "    #(('to', 'TO'), 'pobj', ('us', 'PRP'))]\n",
    "    for outpair in ParseSent:\n",
    "        if outpair[0][1] in Ntag and outpair[2][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[0][0], outpair[2][0])))\n",
    "        elif outpair[2][1] in Ntag and outpair[0][1] in ADJtag:\n",
    "            N_Adj_pair.append(tuple((outpair[2][0], outpair[0][0])))\n",
    "    #N_Adj_pair looks like:\n",
    "    #[('food', 'fair'), ('factor', 'only'), ('deficiencies', 'other')]\n",
    "    \n",
    "    #III-3: detect whether N is AT\n",
    "    AT_ADJ=[]\n",
    "    for pair in N_Adj_pair:\n",
    "        noun=pair[0]\n",
    "        adj=pair[1]\n",
    "        for at in currentAT:\n",
    "            if noun==at or noun in at:\n",
    "                AT_ADJ.append(tuple((at, adj)))\n",
    "    #AT_ADJ looks like:\n",
    "    #[('sushi', 'best'), ('place', 'clean')]\n",
    "    #its noun is real AT, its adj is candidate\n",
    "    #III-4: detect polarity of AT in AT_ADJ by adj\n",
    "    AspectTerm=[]\n",
    "    for p in AT_ADJ:\n",
    "        if p[1] in SentimentDict_Extended['positive']:\n",
    "            AspectTerm.append(tuple((p[0], 'positive')))\n",
    "        elif p[1] in SentimentDict_Extended['negative']:\n",
    "            AspectTerm.append(tuple((p[0], 'negative')))\n",
    "        elif p[1] in SentimentDict_Extended['conflict']:\n",
    "            AspectTerm.append(tuple((p[0], 'conflict')))\n",
    "        else:\n",
    "            AspectTerm.append(tuple((p[0], 'neutral')))\n",
    "    if len(AT_ADJ)!=0:\n",
    "        OutTestRSTDict[str(ID)]['AspectTerm']=AspectTerm\n",
    "        #OutTestRSTDict store the outcome of this program in dictionary form,looks like:\n",
    "        #{'id1':{'sentence':..., 'AspectTerm':[(AT1,ATP1),(AT2,ATP2)], 'AspectCategory':[AC1,AC2]}, 'id2':{}}\n",
    "    else:\n",
    "        blob = TextBlob(currentSent)\n",
    "        tempATP=blob.sentiment.polarity\n",
    "        if tempATP>0:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'positive')))\n",
    "        elif tempATP<0:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'negative')))\n",
    "        else:\n",
    "            for term in currentAT:\n",
    "                AspectTerm.append(tuple((term, 'neutral')))\n",
    "        OutTestRSTDict[str(ID)]['AspectTerm']=AspectTerm   \n",
    "        \n",
    "                    ##############################################\n",
    "                    ############  IV. Write into CSV  ############\n",
    "                    ##############################################               \n",
    "AT_Write=[]\n",
    "AC_Write=[]\n",
    "Sent_Write=[]\n",
    "for ID in range(1,testSentsNum+1):\n",
    "    #if ID>30:\n",
    "        #break\n",
    "    Sent_Write.append(OutTestRSTDict[str(ID)]['sentence'])\n",
    "    sameSentAT=''\n",
    "    sameSentAC=''\n",
    "    for ele1 in OutTestRSTDict[str(ID)]['AspectTerm']:\n",
    "        sameSentAT+='('+ele1[0]+'#'+ele1[1]+')'\n",
    "    for ele2 in OutTestRSTDict[str(ID)]['AspectCategroy']:\n",
    "        sameSentAC+='('+ele2+')'\n",
    "    AT_Write.append(sameSentAT)\n",
    "    AC_Write.append(sameSentAC)\n",
    "        \n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/Subtask2Outcome_restaurant.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1,col2,col3 in zip(AT_Write,AC_Write,Sent_Write):\n",
    "        writer.writerow([str(col1),str(col2),str(col3)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:', stop-start,'seconds')\n",
    "\n",
    "#Subtask2 for restaurant test PhaseB is finished, we got an CSV file with polarities of AT\n",
    "#but need use this CSV file to get outcome for subtask4 for restaurant\n",
    "#that is the final outcome for restaurant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time is: 0.9763410091400146 seconds\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "#Part III of this big program\n",
    "#This program for subtask 4 on restaurant only\n",
    "#We read Subtask2Outcome_restaurant.csv and AT-AC CSV file from subtask3\n",
    "########################################################################\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path_to_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xizhaohan/Desktop/Text Mining/project1/My work/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "start=time()\n",
    "                    #############################################\n",
    "                    ############  I. Read CSV files  ############\n",
    "                    #############################################   \n",
    "\n",
    "#I-1: read Subtask2Outcome_restaurant.csv\n",
    "#I-1-1: get lists\n",
    "task2Sent=[]\n",
    "task2AT=[]\n",
    "task2AC=[]\n",
    "task2SentsNum=0\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/Subtask2Outcome_restaurant.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "    # row is a list:\n",
    "    #['(AT1#ATP1)(AT2#ATP2)', '(AC1)(AC2)', 'sent']\n",
    "        task2Sent.append(row[2])\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        ACraw=row[1].strip('()').split(')(')\n",
    "        samesentAT=[tuple(ele1.split('#')) for ele1 in ATraw]\n",
    "        samesentAC=[ele2 for ele2 in ACraw]\n",
    "        task2AT.append(samesentAT)\n",
    "        task2AC.append(samesentAC)\n",
    "        task2SentsNum+=1\n",
    "file.close()\n",
    "\n",
    "#I-1-2: get total dictionary\n",
    "Task2All_dict={}\n",
    "for ID in range(1,task2SentsNum+1):\n",
    "    #ID in dict form 1 to 800(total number of test sentences)\n",
    "    Task2All_dict[str(ID)]={'sentence':task2Sent[ID-1], 'AspectTerm':task2AT[ID-1], 'AspectCategroy':task2AC[ID-1]}\n",
    "    #Task2All_dict is a 2-D dictionary:\n",
    "    #{'ID1':{'sentence':'sent','AspectTerm':[(AT1,ATP1),(AT2,ATP2)],'AspectCategory':[AC1,AC2]}, 'ID2':{}, ...}\n",
    "\n",
    "#I-2: read AT-AC.csv\n",
    "#I-2-1: get lists\n",
    "AT2AC_AT=[]\n",
    "AT2AC_AC=[]\n",
    "AT2AC_Num=0\n",
    "Label_AC={'1':'food', '2':'service', '3':'price', '4':'ambience', '5':'anecdotes/miscellaneous', '':'anecdotes/miscellaneous'}\n",
    "with open('/Users/xizhaohan/Desktop/Subtask2_4/AT_AC.csv', 'r') as file_AT2AC:\n",
    "    reader_AT2AC = csv.reader(file_AT2AC)\n",
    "    for row in reader_AT2AC:\n",
    "    # row is a list:\n",
    "    #['(AT1)(AT2)', 'AClabel', 'sent'] AClabel is 1,2,3,4,5\n",
    "        ATraw=row[0].strip('()').split(')(')\n",
    "        samesentAT=[ele for ele in ATraw]\n",
    "        AT2AC_AT.append(samesentAT)\n",
    "        AT2AC_AC.append(Label_AC[row[1]])\n",
    "        AT2AC_Num+=1\n",
    "file_AT2AC.close()\n",
    "\n",
    "#I-1-2: get dictionary \n",
    "AT2AC_Dict={'food':[], 'service':[], 'price':[], 'ambience':[],'anecdotes/miscellaneous':[] }\n",
    "for i in range(AT2AC_Num):\n",
    "    AT2AC_Dict[AT2AC_AC[i]]+=AT2AC_AT[i]\n",
    "    #AT2AC_Dict is a dictionray with 5 AC lists, in which contain the corresponding AT.\n",
    "\n",
    "                    ########################################################\n",
    "                    ############  II. Get outcomes of subtask4  ############\n",
    "                    ########################################################\n",
    "\n",
    "FinalDict_RST={}\n",
    "cnt=0\n",
    "for ID in range(1, task2SentsNum+1):\n",
    "    FinalDict_RST[str(ID)]={'sentence':Task2All_dict[str(ID)]['sentence'], 'AspectTerm':[], 'AspectCategroy':[]}\n",
    "    if len(Task2All_dict[str(ID)]['AspectTerm'][0][0])!=0:\n",
    "        FinalDict_RST[str(ID)]['AspectTerm']=Task2All_dict[str(ID)]['AspectTerm']\n",
    "        ATP_SameSent=[pair[1] for pair in Task2All_dict[str(ID)]['AspectTerm']]\n",
    "        if len(list(set(ATP_SameSent)))==1:\n",
    "            ACP=ATP_SameSent[0]\n",
    "            AC_P_SameSent=[tuple((ac, ACP)) for ac in Task2All_dict[str(ID)]['AspectCategroy']]\n",
    "            FinalDict_RST[str(ID)]['AspectCategroy']=AC_P_SameSent\n",
    "        elif len(list(set(ATP_SameSent)))>1:\n",
    "            AC_P_SameSent=[]\n",
    "            for at in Task2All_dict[str(ID)]['AspectTerm']:\n",
    "                currentAC=''\n",
    "                if at[0] in AT2AC_Dict['food']:\n",
    "                    currentAC='food'\n",
    "                elif at[0] in AT2AC_Dict['service']:\n",
    "                    currentAC='service'\n",
    "                elif at[0] in AT2AC_Dict['price']:\n",
    "                    currentAC='price'\n",
    "                elif at[0] in AT2AC_Dict['ambience']:\n",
    "                    currentAC='ambience'\n",
    "                else :\n",
    "                    currentAC='anecdotes/miscellaneous'\n",
    "                currentACP=at[1]\n",
    "                AC_P_SameSent.append(tuple((currentAC, currentACP)))\n",
    "            AC_P_SameSent=list(set(AC_P_SameSent))\n",
    "            FinalDict_RST[str(ID)]['AspectCategroy']=AC_P_SameSent\n",
    "    else:\n",
    "        blob = TextBlob(Task2All_dict[str(ID)]['sentence'])\n",
    "        tempACP=blob.sentiment.polarity\n",
    "        AC_P_SameSent=[]\n",
    "        if tempACP>0:\n",
    "            for ac in Task2All_dict[str(ID)]['AspectCategroy']:\n",
    "                AC_P_SameSent.append(tuple((ac, 'positive')))\n",
    "        elif tempACP<0:\n",
    "            for ac in Task2All_dict[str(ID)]['AspectCategroy']:\n",
    "                AC_P_SameSent.append(tuple((ac, 'negatigve')))\n",
    "        else:\n",
    "            for ac in Task2All_dict[str(ID)]['AspectCategroy']:\n",
    "                AC_P_SameSent.append(tuple((ac, 'neutral')))\n",
    "        FinalDict_RST[str(ID)]['AspectCategroy']=AC_P_SameSent\n",
    "    FinalDict_RST[str(ID)]['AspectTerm']=list(set(FinalDict_RST[str(ID)]['AspectTerm']))\n",
    "    FinalDict_RST[str(ID)]['AspectCategroy']=list(set(FinalDict_RST[str(ID)]['AspectCategroy']))\n",
    "                    \n",
    "                    ###############################################\n",
    "                    ############  III. Write into CSV  ############\n",
    "                    ###############################################              \n",
    "AT_Write=[]\n",
    "AC_Write=[]\n",
    "Sent_Write=[]\n",
    "for ID in range(1,task2SentsNum+1):\n",
    "    Sent_Write.append(FinalDict_RST[str(ID)]['sentence'])\n",
    "    sameSentAT=''\n",
    "    sameSentAC=''\n",
    "    for ele1 in FinalDict_RST[str(ID)]['AspectTerm']:\n",
    "        sameSentAT+='('+ele1[0]+'#'+ele1[1]+')'\n",
    "    for ele2 in FinalDict_RST[str(ID)]['AspectCategroy']:\n",
    "        sameSentAC+='('+ele2[0]+'#'+ele2[1]+')'\n",
    "    AT_Write.append(sameSentAT)\n",
    "    AC_Write.append(sameSentAC)\n",
    "\n",
    "with open( '/Users/xizhaohan/Desktop/Subtask2_4/Subtask4_FinalOutcome_restaurant.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for col1,col2,col3 in zip(AT_Write,AC_Write,Sent_Write):\n",
    "        writer.writerow([str(col1),str(col2),str(col3)])\n",
    "file.close()\n",
    "\n",
    "stop=time()\n",
    "print('Total time is:', stop-start,'seconds')\n",
    "\n",
    "#Subtask4 for restaurant test PhaseB is finished, we got an CSV file with polarities of AT,AC\n",
    "#This file is the final outcome for restaurant, containing outcomes of both subtask2 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
